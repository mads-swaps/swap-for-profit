{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from datetime import datetime\n",
    "import pytz\n",
    "import re\n",
    "from operator import itemgetter\n",
    "\n",
    "pd.set_option('display.max_columns', 999)\n",
    "\n",
    "VALID_FEATURES = [\n",
    "    'pair_id','open_time','close_time','dow','tod',\n",
    "    'open','high','low','close',\n",
    "    'number_of_trades','volume','quote_asset_volume','taker_buy_base_asset_volume','taker_buy_quote_asset_volume',\n",
    "    'ma14','ma30','ma90',\n",
    "    'sup14','sup30','sup90',\n",
    "    'res14','res30','res90',\n",
    "    'atr','atr_diff','atr_ma14',\n",
    "    'rsi','rsi_diff','rsi_ma14',\n",
    "    'trend_up','trend_up3','trend_up14','trend_up30',\n",
    "    'cs_ss','cs_ssr','cs_hm','cs_hmr','cs_brh','cs_buh','cs_ebu','cs_ebr'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PostgreSQL connection data taken from hidden.py\n"
     ]
    }
   ],
   "source": [
    "# local postgres connection only\n",
    "import hidden\n",
    "sql_string = hidden.psycopg2(hidden.secrets())\n",
    "print('PostgreSQL connection data taken from hidden.py')\n",
    "\n",
    "# Make the connection and cursor\n",
    "conn = psycopg2.connect(sql_string, connect_timeout=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are only used for caching\n",
    "import hashlib\n",
    "import pickle\n",
    "\n",
    "def get_batch_data(base_coin, quote_coin, start_time, end_time, columns, batch_size=30000, extra_rows=0, use_cache=True):\n",
    "    column_info = [(x,) + re.match('^(?P<feature>[a-z][a-z0-9]*(?:_[a-z][a-z0-9]*)*)(?:_(?P<shift>[0-9]{1,3}))?$',x).groups() for x in columns]\n",
    "    max_lookback = max([(0 if x==None else int(x)) for _,_,x in column_info])\n",
    "    s = base_coin+quote_coin+f\"{start_time}\"+f\"{end_time}\"+\"\".join(columns)+str(batch_size)+str(extra_rows)\n",
    "    h = hashlib.md5(s.encode('utf-8')).hexdigest()\n",
    "    should_use_cache = use_cache and (datetime.strptime(end_time, '%Y-%m-%d') < datetime.now(pytz.timezone('UTC')).replace(tzinfo=None))\n",
    "    if should_use_cache:\n",
    "        # Can use cache\n",
    "        try:\n",
    "            with open(f'../data/t2/{h}.pkl', 'rb') as fp:\n",
    "                print(f\"Using cache file: ../data/t2/{h}.pkl\")\n",
    "                return pickle.load(fp)\n",
    "        except:\n",
    "            print(f\"No cache found\")\n",
    "            pass\n",
    "\n",
    "    sql = f\"\"\"\n",
    "select\n",
    "    f.*, open_time, open, high, low, close, volume, close_time, quote_asset_volume, number_of_trades, taker_buy_base_asset_volume, taker_buy_quote_asset_volume\t\n",
    "from\n",
    "    (\n",
    "        (select * from (select id as the_pair from pairs p where p.coin1='{base_coin}' and p.coin2='{quote_coin}') z inner join candlestick_15m on the_pair=pair_id where close_time notnull and open_time < '{start_time}' order by open_time desc limit {max_lookback + extra_rows})\n",
    "            union all\n",
    "        (select * from (select id as the_pair from pairs p where p.coin1='{base_coin}' and p.coin2='{quote_coin}') z inner join candlestick_15m on the_pair=pair_id where close_time notnull and open_time between '{start_time}' and '{end_time}' order by open_time limit {batch_size})\n",
    "    ) cm\n",
    "inner join \n",
    "    features f on f.pair_id = cm.pair_id and f.candle_open_time = cm.open_time\n",
    "order by\n",
    "    open_time desc\n",
    "\"\"\"\n",
    "    base_df = pd.read_sql_query(sql, conn)\n",
    "    df = base_df[['candle_open_time']].copy()\n",
    "    for name, feature, shift in column_info:\n",
    "        assert feature in VALID_FEATURES, f\"Invalid feature: {feature} for {name}\"\n",
    "        df[name] = base_df[feature].shift((0 if shift==None else -int(shift)))\n",
    "        \n",
    "    if extra_rows == 0:\n",
    "        extra_df = None\n",
    "    else:\n",
    "        extra_df = df.copy()\n",
    "        extra_df['is_extra'] = ~extra_df['candle_open_time'].between(start_time, end_time)\n",
    "        extra_df = extra_df.set_index('candle_open_time').sort_index()\n",
    "        \n",
    "    df = df[df['candle_open_time'].between(start_time, end_time)]\n",
    "    df = df.set_index('candle_open_time').sort_index()\n",
    "\n",
    "    ref_df = base_df[['open_time','open', 'high', 'low', 'close']].copy()\n",
    "    ref_df = ref_df[ref_df['open_time'].between(start_time, end_time)]\n",
    "    ref_df = ref_df.set_index('open_time').sort_index()\n",
    "\n",
    "    batch_close_time = base_df['close_time'].max()\n",
    "    \n",
    "    if should_use_cache:\n",
    "        print(f\"Saving cache to: ../data/t2/{h}.pkl\")\n",
    "        with open(f'../data/t2/{h}.pkl', 'wb') as fp:\n",
    "            pickle.dump((df, ref_df, extra_df, batch_close_time), fp, protocol=4)\n",
    "\n",
    "    return df, ref_df, extra_df, batch_close_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "static_columns = ['open']\n",
    "repeat_columns = ['high', 'low', 'close', 'rsi', 'trend_up3','trend_up14', \n",
    "                  'cs_ss','cs_ssr','cs_hm','cs_hmr','cs_ebu','cs_ebr']\n",
    "\n",
    "columns = static_columns + [f\"{rc}_{i}\" for rc in repeat_columns for i in range(0,8)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#len(repeat_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping = {\n",
    "    'ETHBTC':0,\n",
    "    'BTCUSDT':1,\n",
    "    'ETHUSDT':2,\n",
    "    'BTCETH':-1,\n",
    "    'USDTBTC':-2,\n",
    "    'USDTETH':-3\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cache file: ../data/t2/8ab4181b1214fa6a2afce44ba627789e.pkl\n",
      "Using cache file: ../data/t2/a84bfae4281950e643216f415fc2075c.pkl\n",
      "Using cache file: ../data/t2/ab0b63fbc788e447eb424e4bbb7e44fb.pkl\n",
      "Using cache file: ../data/t2/c4a9096f8ad174bd1cdc357c444c3007.pkl\n",
      "Using cache file: ../data/t2/eb764641cee5900041fbe5e702d4b715.pkl\n",
      "Using cache file: ../data/t2/7914487a7ea3f192f96a3f0faf0cd739.pkl\n"
     ]
    }
   ],
   "source": [
    "from itertools import permutations\n",
    "\n",
    "# download raw data pre-processing, this will be the same data that the simulator receives\n",
    "\n",
    "for a,b in permutations(['ETH','BTC','USDT'],2):\n",
    "    df, ref_df, _, _ = get_batch_data(a, b, '2018-01-01', '2021-08-07', columns, 500000, 0, True)\n",
    "    df = df.astype(float)\n",
    "    \n",
    "    with open(f'../data/t2_{a}_{b}.pkl', 'wb') as fp:\n",
    "        pickle.dump(df, fp, protocol=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# additional ETL here, like normalization, scaling and other stuff, also create rows for target use\n",
    "\n",
    "dfs = []\n",
    "\n",
    "for a,b in permutations(['ETH','BTC','USDT'],2):\n",
    "    with open(f'../data/t2_{a}_{b}.pkl', 'rb') as fp:\n",
    "        df = pickle.load(fp)\n",
    "        df = df.astype(float)\n",
    "    \n",
    "        for c in df.columns:\n",
    "            if c.startswith(\"trend_up\"):\n",
    "                d = 'tc2x_' + c.replace('_up','')\n",
    "                df[d] = (df[c] - 0.5) * 2\n",
    "            elif c.startswith(\"rsi_\"):\n",
    "                df['tc2x_'+c] = (df[c] - 50) / 50\n",
    "            elif c.startswith(\"high_\") or c.startswith(\"low_\") or c.startswith(\"close_\"):\n",
    "                df['tc2x_'+c] = ((df[c] / df['open']) - 1) * 30\n",
    "        \n",
    "        flag_pairs = [('tc2x_ss','cs_ss','cs_ssr'),('tc2x_hm','cs_hm','cs_hmr'),('tc2x_eb','cs_ebu','cs_ebr')]\n",
    "        \n",
    "        for newp,p1,p2 in flag_pairs:\n",
    "             for i in range(0,8):\n",
    "                    df[f\"{newp}_{i}\"] = df[f\"{p1}_{i}\"] - df[f\"{p2}_{i}\"]\n",
    "        \n",
    "        shifts = []\n",
    "        for i in range(1,96):\n",
    "            shifts.append(((df['close_0'].shift(-i) / df['open']) - 1))\n",
    "            #df[f'tc2y_close_{i}'] =((df['close_0'].shift(-i) / df['open']) - 1) * 30\n",
    "        shift_df = pd.concat(shifts, axis=1).dropna()\n",
    "        shift_df[(shift_df > -0.01) & (shift_df < 0.03)] = np.nan\n",
    "        y = shift_df.bfill(axis=1).iloc[:,0] > 0\n",
    "        \n",
    "        df = df[[c for c in df.columns if c.startswith(\"tc2\")]]\n",
    "        df = df.clip(-1,1)\n",
    "        df['y'] = y.astype(float)\n",
    "        df['pair_id'] = mapping[f\"{a}{b}\"]\n",
    "        df = df.reset_index().set_index(['pair_id','candle_open_time']).dropna()\n",
    "        \n",
    "        dfs.append(df)\n",
    "\n",
    "df = pd.concat(dfs)\n",
    "dfs = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.00    0.0\n",
       "0.05    0.0\n",
       "0.10    0.0\n",
       "0.15    0.0\n",
       "0.20    0.0\n",
       "0.25    0.0\n",
       "0.30    0.0\n",
       "0.35    0.0\n",
       "0.40    0.0\n",
       "0.45    0.0\n",
       "0.50    0.0\n",
       "0.55    0.0\n",
       "0.60    0.0\n",
       "0.65    0.0\n",
       "0.70    0.0\n",
       "0.75    0.0\n",
       "0.80    1.0\n",
       "0.85    1.0\n",
       "0.90    1.0\n",
       "0.95    1.0\n",
       "1.00    1.0\n",
       "Name: y, dtype: float64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['y'].quantile(np.linspace(0.0, 1.0, num=21))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'../data/t2/full.pkl', 'wb') as fp:\n",
    "    pickle.dump(df, fp, protocol=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'../data/t2/train.pkl', 'wb') as fp:\n",
    "    pickle.dump((df[df.index.get_level_values(1) < '2021-01-01']).reset_index(drop=True), fp, protocol=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'../data/t2/test.pkl', 'wb') as fp:\n",
    "    pickle.dump((df[df.index.get_level_values(1) >= '2021-01-01']).reset_index(drop=True), fp, protocol=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
