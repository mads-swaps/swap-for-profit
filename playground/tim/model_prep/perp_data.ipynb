{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from datetime import datetime\n",
    "import pytz\n",
    "import re\n",
    "from operator import itemgetter\n",
    "\n",
    "\n",
    "VALID_FEATURES = [\n",
    "    'pair_id','open_time','close_time','dow','tod',\n",
    "    'open','high','low','close',\n",
    "    'number_of_trades','volume','quote_asset_volume','taker_buy_base_asset_volume','taker_buy_quote_asset_volume',\n",
    "    'ma14','ma30','ma90',\n",
    "    'sup14','sup30','sup90',\n",
    "    'res14','res30','res90',\n",
    "    'atr','atr_diff','atr_ma14',\n",
    "    'rsi','rsi_diff','rsi_ma14',\n",
    "    'trend_up','trend_up3','trend_up14','trend_up30',\n",
    "    'cs_ss','cs_ssr','cs_hm','cs_hmr','cs_brh','cs_buh','cs_ebu','cs_ebr'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PostgreSQL connection data taken from hidden.py\n"
     ]
    }
   ],
   "source": [
    "# local postgres connection only\n",
    "import hidden\n",
    "sql_string = hidden.psycopg2(hidden.secrets())\n",
    "print('PostgreSQL connection data taken from hidden.py')\n",
    "\n",
    "# Make the connection and cursor\n",
    "conn = psycopg2.connect(sql_string, connect_timeout=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are only used for caching\n",
    "import hashlib\n",
    "import pickle\n",
    "\n",
    "def get_batch_data(base_coin, quote_coin, start_time, end_time, columns, batch_size=30000, extra_rows=0, use_cache=True):\n",
    "    column_info = [(x,) + re.match('^(?P<feature>[a-z][a-z0-9]*(?:_[a-z][a-z0-9]*)*)(?:_(?P<shift>[0-9]{1,3}))?$',x).groups() for x in columns]\n",
    "    max_lookback = max([(0 if x==None else int(x)) for _,_,x in column_info])\n",
    "    s = base_coin+quote_coin+f\"{start_time}\"+f\"{end_time}\"+\"\".join(columns)+str(batch_size)+str(extra_rows)\n",
    "    h = hashlib.md5(s.encode('utf-8')).hexdigest()\n",
    "    should_use_cache = use_cache and (datetime.strptime(end_time, '%Y-%m-%d') < datetime.now(pytz.timezone('UTC')).replace(tzinfo=None))\n",
    "    if should_use_cache:\n",
    "        # Can use cache\n",
    "        try:\n",
    "            with open(f'./cache_data/{h}.pkl', 'rb') as fp:\n",
    "                print(f\"Using cache file: ./cache_data/{h}.pkl\")\n",
    "                return pickle.load(fp)\n",
    "        except:\n",
    "            print(f\"No cache found\")\n",
    "            pass\n",
    "\n",
    "    sql = f\"\"\"\n",
    "select\n",
    "    f.*, open_time, open, high, low, close, volume, close_time, quote_asset_volume, number_of_trades, taker_buy_base_asset_volume, taker_buy_quote_asset_volume\t\n",
    "from\n",
    "    (\n",
    "        (select * from (select id as the_pair from pairs p where p.coin1='{base_coin}' and p.coin2='{quote_coin}') z inner join candlestick_15m on the_pair=pair_id where close_time notnull and open_time < '{start_time}' order by open_time desc limit {max_lookback + extra_rows})\n",
    "            union all\n",
    "        (select * from (select id as the_pair from pairs p where p.coin1='{base_coin}' and p.coin2='{quote_coin}') z inner join candlestick_15m on the_pair=pair_id where close_time notnull and open_time between '{start_time}' and '{end_time}' order by open_time limit {batch_size})\n",
    "    ) cm\n",
    "inner join \n",
    "    features f on f.pair_id = cm.pair_id and f.candle_open_time = cm.open_time\n",
    "order by\n",
    "    open_time desc\n",
    "\"\"\"\n",
    "    base_df = pd.read_sql_query(sql, conn)\n",
    "    df = base_df[['candle_open_time']].copy()\n",
    "    for name, feature, shift in column_info:\n",
    "        assert feature in VALID_FEATURES, f\"Invalid feature: {feature} for {name}\"\n",
    "        df[name] = base_df[feature].shift((0 if shift==None else -int(shift)))\n",
    "        \n",
    "    if extra_rows == 0:\n",
    "        extra_df = None\n",
    "    else:\n",
    "        extra_df = df.copy()\n",
    "        extra_df['is_extra'] = ~extra_df['candle_open_time'].between(start_time, end_time)\n",
    "        extra_df = extra_df.set_index('candle_open_time').sort_index()\n",
    "        \n",
    "    df = df[df['candle_open_time'].between(start_time, end_time)]\n",
    "    df = df.set_index('candle_open_time').sort_index()\n",
    "\n",
    "    ref_df = base_df[['open_time','open', 'high', 'low', 'close']].copy()\n",
    "    ref_df = ref_df[ref_df['open_time'].between(start_time, end_time)]\n",
    "    ref_df = ref_df.set_index('open_time').sort_index()\n",
    "\n",
    "    batch_close_time = base_df['close_time'].max()\n",
    "    \n",
    "    if should_use_cache:\n",
    "        with open(f'./cache_data/{h}.pkl', 'wb') as fp:\n",
    "            print(f\"Saving cache to: ./cache_data/{h}.pkl\")\n",
    "            pickle.dump((df, ref_df, extra_df, batch_close_time), fp, protocol=4)\n",
    "\n",
    "    return df, ref_df, extra_df, batch_close_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No cache found\n",
      "Saving cache to: ./cache_data/ad0703e34ece2cf2318333bf389d12c4.pkl\n"
     ]
    }
   ],
   "source": [
    "df, ref_df, extra_df, batch_close_time = get_batch_data('ETH', 'BTC', '2018-01-01', '2021-07-31', VALID_FEATURES, 50000, 0, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "VALID_FEATURES = [\n",
    "    'pair_id','open_time','close_time','dow','tod',\n",
    "    'open','high','low','close',\n",
    "    'number_of_trades','volume','quote_asset_volume','taker_buy_base_asset_volume','taker_buy_quote_asset_volume',\n",
    "    'ma14','ma30','ma90',\n",
    "    'sup14','sup30','sup90',\n",
    "    'res14','res30','res90',\n",
    "    'atr','atr_diff','atr_ma14',\n",
    "    'rsi','rsi_diff','rsi_ma14',\n",
    "    'trend_up','trend_up3','trend_up14','trend_up30',\n",
    "    'cs_ss','cs_ssr','cs_hm','cs_hmr','cs_brh','cs_buh','cs_ebu','cs_ebr'\n",
    "]\n",
    "\n",
    "normalize_columns = [\n",
    "    'open','high','low','close','quote_asset_volume','taker_buy_quote_asset_volume',\n",
    "    'ma14','ma30','ma90',\n",
    "    'sup14','sup30','sup90',\n",
    "    'res14','res30','res90',\n",
    "    'atr','atr_diff','atr_ma14',\n",
    "]\n",
    "\n",
    "inverse_normalize_columns = [\n",
    "    'volume','taker_buy_base_asset_volume'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[inverse_normalize_columns] = df[inverse_normalize_columns] * df.open\n",
    "df[normalize_columns] = df[normalize_columns] / df.open\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
