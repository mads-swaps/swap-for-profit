{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from datetime import datetime\n",
    "import pytz\n",
    "import re\n",
    "from operator import itemgetter\n",
    "\n",
    "\n",
    "VALID_FEATURES = [\n",
    "    'pair_id','open_time','close_time','dow','tod',\n",
    "    'open','high','low','close',\n",
    "    'number_of_trades','volume','quote_asset_volume','taker_buy_base_asset_volume','taker_buy_quote_asset_volume',\n",
    "    'ma14','ma30','ma90',\n",
    "    'sup14','sup30','sup90',\n",
    "    'res14','res30','res90',\n",
    "    'atr','atr_diff','atr_ma14',\n",
    "    'rsi','rsi_diff','rsi_ma14',\n",
    "    'trend_up','trend_up3','trend_up14','trend_up30',\n",
    "    'cs_ss','cs_ssr','cs_hm','cs_hmr','cs_brh','cs_buh','cs_ebu','cs_ebr'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PostgreSQL connection data taken from hidden.py\n"
     ]
    }
   ],
   "source": [
    "# local postgres connection only\n",
    "import hidden\n",
    "sql_string = hidden.psycopg2(hidden.secrets())\n",
    "print('PostgreSQL connection data taken from hidden.py')\n",
    "\n",
    "# Make the connection and cursor\n",
    "conn = psycopg2.connect(sql_string, connect_timeout=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are only used for caching\n",
    "import hashlib\n",
    "import pickle\n",
    "\n",
    "def get_batch_data(base_coin, quote_coin, start_time, end_time, columns, batch_size=30000, extra_rows=0, use_cache=True):\n",
    "    column_info = [(x,) + re.match('^(?P<feature>[a-z][a-z0-9]*(?:_[a-z][a-z0-9]*)*)(?:_(?P<shift>[0-9]{1,3}))?$',x).groups() for x in columns]\n",
    "    max_lookback = max([(0 if x==None else int(x)) for _,_,x in column_info])\n",
    "    s = base_coin+quote_coin+f\"{start_time}\"+f\"{end_time}\"+\"\".join(columns)+str(batch_size)+str(extra_rows)\n",
    "    h = hashlib.md5(s.encode('utf-8')).hexdigest()\n",
    "    should_use_cache = use_cache and (datetime.strptime(end_time, '%Y-%m-%d') < datetime.now(pytz.timezone('UTC')).replace(tzinfo=None))\n",
    "    if should_use_cache:\n",
    "        # Can use cache\n",
    "        try:\n",
    "            with open(f'./cache_data/{h}.pkl', 'rb') as fp:\n",
    "                print(f\"Using cache file: ./cache_data/{h}.pkl\")\n",
    "                return pickle.load(fp)\n",
    "        except:\n",
    "            print(f\"No cache found\")\n",
    "            pass\n",
    "\n",
    "    sql = f\"\"\"\n",
    "select\n",
    "    f.*, open_time, open, high, low, close, volume, close_time, quote_asset_volume, number_of_trades, taker_buy_base_asset_volume, taker_buy_quote_asset_volume\t\n",
    "from\n",
    "    (\n",
    "        (select * from (select id as the_pair from pairs p where p.coin1='{base_coin}' and p.coin2='{quote_coin}') z inner join candlestick_15m on the_pair=pair_id where close_time notnull and open_time < '{start_time}' order by open_time desc limit {max_lookback + extra_rows})\n",
    "            union all\n",
    "        (select * from (select id as the_pair from pairs p where p.coin1='{base_coin}' and p.coin2='{quote_coin}') z inner join candlestick_15m on the_pair=pair_id where close_time notnull and open_time between '{start_time}' and '{end_time}' order by open_time limit {batch_size})\n",
    "    ) cm\n",
    "inner join \n",
    "    features f on f.pair_id = cm.pair_id and f.candle_open_time = cm.open_time\n",
    "order by\n",
    "    open_time desc\n",
    "\"\"\"\n",
    "    base_df = pd.read_sql_query(sql, conn)\n",
    "    df = base_df[['candle_open_time']].copy()\n",
    "    for name, feature, shift in column_info:\n",
    "        assert feature in VALID_FEATURES, f\"Invalid feature: {feature} for {name}\"\n",
    "        df[name] = base_df[feature].shift((0 if shift==None else -int(shift)))\n",
    "        \n",
    "    if extra_rows == 0:\n",
    "        extra_df = None\n",
    "    else:\n",
    "        extra_df = df.copy()\n",
    "        extra_df['is_extra'] = ~extra_df['candle_open_time'].between(start_time, end_time)\n",
    "        extra_df = extra_df.set_index('candle_open_time').sort_index()\n",
    "        \n",
    "    df = df[df['candle_open_time'].between(start_time, end_time)]\n",
    "    df = df.set_index('candle_open_time').sort_index()\n",
    "\n",
    "    ref_df = base_df[['open_time','open', 'high', 'low', 'close']].copy()\n",
    "    ref_df = ref_df[ref_df['open_time'].between(start_time, end_time)]\n",
    "    ref_df = ref_df.set_index('open_time').sort_index()\n",
    "\n",
    "    batch_close_time = base_df['close_time'].max()\n",
    "    \n",
    "    if should_use_cache:\n",
    "        with open(f'./cache_data/{h}.pkl', 'wb') as fp:\n",
    "            print(f\"Saving cache to: ./cache_data/{h}.pkl\")\n",
    "            pickle.dump((df, ref_df, extra_df, batch_close_time), fp, protocol=4)\n",
    "\n",
    "    return df, ref_df, extra_df, batch_close_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "repeat_columns = ['high', 'low', 'close','rsi',\n",
    "    'trend_up','trend_up3','trend_up14','trend_up30',\n",
    "    'cs_ss','cs_ssr','cs_hm','cs_hmr','cs_brh','cs_buh','cs_ebu','cs_ebr']\n",
    "static_columns = ['open']\n",
    "\n",
    "columns = static_columns + [f\"{rc}_{i}\" for rc in repeat_columns for i in range(0,24)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['high',\n",
       " 'low',\n",
       " 'close',\n",
       " 'rsi',\n",
       " 'trend_up',\n",
       " 'trend_up3',\n",
       " 'trend_up14',\n",
       " 'trend_up30',\n",
       " 'cs_ss',\n",
       " 'cs_ssr',\n",
       " 'cs_hm',\n",
       " 'cs_hmr',\n",
       " 'cs_brh',\n",
       " 'cs_buh',\n",
       " 'cs_ebu',\n",
       " 'cs_ebr']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "repeat_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(repeat_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping = {\n",
    "    'ETHBTC':0,\n",
    "    'BTCUSDT':1,\n",
    "    'ETHUSDT':2,\n",
    "    'BTCETH':-1,\n",
    "    'USDTBTC':-2,\n",
    "    'USDTETH':-3\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No cache found\n",
      "Saving cache to: ./cache_data/31540be8258b739d4961aebd9d62ea1c.pkl\n",
      "No cache found\n",
      "Saving cache to: ./cache_data/cd3862dfd26f3666dc1bf3b6c90e6aee.pkl\n",
      "No cache found\n",
      "Saving cache to: ./cache_data/0246979fa9db8a28a785d5c6bcf9dfce.pkl\n",
      "No cache found\n",
      "Saving cache to: ./cache_data/85478b2a84ed13059d45744fd2805e3e.pkl\n",
      "No cache found\n",
      "Saving cache to: ./cache_data/441f88e6601c92c89c79a631532c2c51.pkl\n",
      "No cache found\n",
      "Saving cache to: ./cache_data/8c723b2ec136f6dcfbf3a79ddc3ebf96.pkl\n"
     ]
    }
   ],
   "source": [
    "from itertools import permutations\n",
    "\n",
    "for a,b in permutations(['ETH','BTC','USDT'],2):\n",
    "    df, ref_df, _, _ = get_batch_data(a, b, '2018-01-01', '2021-08-07', columns, 500000, 0, True)\n",
    "    df[[\"r_\"+x for x in df.columns if x.startswith('rsi_')]] = df[[x for x in df.columns if x.startswith('rsi_')]] < 30\n",
    "    df[[x for x in df.columns if x.startswith('rsi_')]] = df[[x for x in df.columns if x.startswith('rsi_')]] > 70\n",
    "\n",
    "#     for col in df.columns:\n",
    "#         if df[col].dtype.kind in ['O']:\n",
    "#             df[col] = df[col].astype(float)\n",
    "    df = df.astype(float)\n",
    "            \n",
    "    with open(f'./cache_data/{a}_{b}_f4_406.pkl', 'wb') as fp:\n",
    "        pickle.dump(df, fp, protocol=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'./cache_data/targets.pkl', 'rb') as fp:\n",
    "    y_df = pickle.load(fp)\n",
    "    y_df = y_df.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pair_id</th>\n",
       "      <th>open_time</th>\n",
       "      <th>close__ewm_8</th>\n",
       "      <th>close__ewm_24</th>\n",
       "      <th>close__ewm_48</th>\n",
       "      <th>close__ewm_96</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2017-07-14 04:00:00</td>\n",
       "      <td>-0.085983</td>\n",
       "      <td>0.314472</td>\n",
       "      <td>0.223167</td>\n",
       "      <td>0.589414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>2017-07-14 04:15:00</td>\n",
       "      <td>0.094960</td>\n",
       "      <td>0.472451</td>\n",
       "      <td>0.347574</td>\n",
       "      <td>0.742649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>2017-07-14 04:30:00</td>\n",
       "      <td>0.024124</td>\n",
       "      <td>0.372090</td>\n",
       "      <td>0.221948</td>\n",
       "      <td>0.632886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>2017-07-14 04:45:00</td>\n",
       "      <td>0.056716</td>\n",
       "      <td>0.376667</td>\n",
       "      <td>0.209570</td>\n",
       "      <td>0.636785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>2017-07-14 05:00:00</td>\n",
       "      <td>0.503308</td>\n",
       "      <td>0.793327</td>\n",
       "      <td>0.602286</td>\n",
       "      <td>1.053142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>839094</th>\n",
       "      <td>1</td>\n",
       "      <td>2021-08-06 22:45:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>839095</th>\n",
       "      <td>1</td>\n",
       "      <td>2021-08-06 23:00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>839096</th>\n",
       "      <td>1</td>\n",
       "      <td>2021-08-06 23:15:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>839097</th>\n",
       "      <td>1</td>\n",
       "      <td>2021-08-06 23:30:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>839098</th>\n",
       "      <td>1</td>\n",
       "      <td>2021-08-06 23:45:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>839099 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        pair_id           open_time  close__ewm_8  close__ewm_24  \\\n",
       "0             0 2017-07-14 04:00:00     -0.085983       0.314472   \n",
       "1             0 2017-07-14 04:15:00      0.094960       0.472451   \n",
       "2             0 2017-07-14 04:30:00      0.024124       0.372090   \n",
       "3             0 2017-07-14 04:45:00      0.056716       0.376667   \n",
       "4             0 2017-07-14 05:00:00      0.503308       0.793327   \n",
       "...         ...                 ...           ...            ...   \n",
       "839094        1 2021-08-06 22:45:00           NaN            NaN   \n",
       "839095        1 2021-08-06 23:00:00           NaN            NaN   \n",
       "839096        1 2021-08-06 23:15:00           NaN            NaN   \n",
       "839097        1 2021-08-06 23:30:00           NaN            NaN   \n",
       "839098        1 2021-08-06 23:45:00           NaN            NaN   \n",
       "\n",
       "        close__ewm_48  close__ewm_96  \n",
       "0            0.223167       0.589414  \n",
       "1            0.347574       0.742649  \n",
       "2            0.221948       0.632886  \n",
       "3            0.209570       0.636785  \n",
       "4            0.602286       1.053142  \n",
       "...               ...            ...  \n",
       "839094            NaN            NaN  \n",
       "839095            NaN            NaN  \n",
       "839096            NaN            NaN  \n",
       "839097            NaN            NaN  \n",
       "839098            NaN            NaN  \n",
       "\n",
       "[839099 rows x 6 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = []\n",
    "for a,b in permutations(['ETH','BTC','USDT'],2):\n",
    "    with open(f'./cache_data/{a}_{b}_f4_406.pkl', 'rb') as fp:\n",
    "        df = pickle.load(fp)\n",
    "        df['pair_id'] = mapping[f\"{a}{b}\"]\n",
    "        df = df.reset_index().set_index(['pair_id','candle_open_time'])\n",
    "        df = df.merge(y_df, left_on=['pair_id','candle_open_time'], right_on=['pair_id','open_time'])\n",
    "        dfs.append(df)\n",
    "merged_df = pd.concat(dfs)\n",
    "merged_df = merged_df.dropna()\n",
    "merged_df = merged_df.set_index(['pair_id','open_time'])\n",
    "dfs = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'../data/tc0_full.pkl', 'wb') as fp:\n",
    "    pickle.dump(merged_df, fp, protocol=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'../data/tc0_train.pkl', 'wb') as fp:\n",
    "    pickle.dump((merged_df[merged_df.index.get_level_values(1) < '2021-01-01']).reset_index(drop=True), fp, protocol=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'../data/tc0_test.pkl', 'wb') as fp:\n",
    "    pickle.dump((merged_df[merged_df.index.get_level_values(1) >= '2021-01-01']).reset_index(drop=True), fp, protocol=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['r_rsi_0',\n",
       " 'r_rsi_1',\n",
       " 'r_rsi_2',\n",
       " 'r_rsi_3',\n",
       " 'r_rsi_4',\n",
       " 'r_rsi_5',\n",
       " 'r_rsi_6',\n",
       " 'r_rsi_7',\n",
       " 'r_rsi_8',\n",
       " 'r_rsi_9',\n",
       " 'r_rsi_10',\n",
       " 'r_rsi_11',\n",
       " 'r_rsi_12',\n",
       " 'r_rsi_13',\n",
       " 'r_rsi_14',\n",
       " 'r_rsi_15',\n",
       " 'r_rsi_16',\n",
       " 'r_rsi_17',\n",
       " 'r_rsi_18',\n",
       " 'r_rsi_19',\n",
       " 'r_rsi_20',\n",
       " 'r_rsi_21',\n",
       " 'r_rsi_22',\n",
       " 'r_rsi_23',\n",
       " 'close__ewm_8',\n",
       " 'close__ewm_24',\n",
       " 'close__ewm_48',\n",
       " 'close__ewm_96']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[x for x in merged_df.columns if x not in columns]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
