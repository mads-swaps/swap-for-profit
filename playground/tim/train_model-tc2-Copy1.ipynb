{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import random\n",
    "import train\n",
    "from model import NNModelEx, NNModelBCE\n",
    "\n",
    "pd.set_option('display.max_columns', 999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For this model, the data preprocessing part is already completed with the exception of scaling.\n",
    "# so we just need to scale here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ref_X_y(df):\n",
    "    X_cols = [c for c in df.columns if c.startswith('tc2x_')]\n",
    "    y_cols = [c for c in df.columns if c.startswith('y')]\n",
    "    return (df[X_cols], df[y_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = {} # loads raw data and stores as a dict cache\n",
    "\n",
    "def dataset_key(dataset='', validation=False):\n",
    "    return dataset+('test' if validation else 'train')\n",
    "\n",
    "\n",
    "def load_data(raw, dataset='', validation=False):\n",
    "    '''\n",
    "    Return dataframe matching data set and validation. Dictionary input will be updated.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    raw : dict\n",
    "        dictionary which caches the dataframes and will be updated accordingly\n",
    "\n",
    "    dataset : str\n",
    "        which dataset to use? valid input includes: empty str for full set, sample_, and secret_\n",
    "\n",
    "    validation : bool\n",
    "        load validation set? if true then use _test, otherwise use _train.  Note secret_ doesn't have _train\n",
    "    '''\n",
    "    key = dataset+('test' if validation else 'train')\n",
    "    if key not in raw:\n",
    "        print(f\"Loading data to cache for: {key}\")\n",
    "        raw[key] = pd.read_pickle(f'./data/{key}.pkl')\n",
    "    return raw[key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "configurations = {\n",
    "    'dataset' : 't2/', # '', 'sample_', 'secret_'\n",
    "    'model_identifier' : \"tc2_2\",\n",
    "    'model_path' : f\"./models\",\n",
    "    'model': NNModelBCE,\n",
    "    'device' : 'cpu',\n",
    "    'random_seed' : 0,\n",
    "    'lr' : 3e-3,\n",
    "    'weight_decay' : 0.3, #Adam\n",
    "    'max_epochs' : 50000,\n",
    "    'do_validate' : True,\n",
    "    'model_definition' : [\n",
    "        ('l', (500,)), ('r', (True,)),\n",
    "        ('l', (500,)), ('r', (True,)),\n",
    "        ('l', (500,)), ('r', (True,)),\n",
    "        ('l', (500,)), ('r', (True,)),\n",
    "        ('l', (500,)), ('r', (True,)),\n",
    "        ('l', (500,)), ('r', (True,)),\n",
    "        ('l', (500,)), ('r', (True,)),\n",
    "        ('l', (500,)), ('r', (True,)),\n",
    "    ],\n",
    "    'train_params' : {\n",
    "        'batch_size': 10000,\n",
    "        'shuffle': True,\n",
    "        'num_workers': 3,\n",
    "        'pin_memory': True,\n",
    "    },\n",
    "    'test_params' : {\n",
    "        'batch_size': 200000,\n",
    "        'num_workers': 1,\n",
    "        'pin_memory': True,\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data to cache for: t2/train\n",
      "Loading data to cache for: t2/test\n",
      "CPU times: user 150 ms, sys: 349 ms, total: 499 ms\n",
      "Wall time: 498 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "train_df = load_data(raw_data,dataset=configurations['dataset'],validation=False)\n",
    "test_df = load_data(raw_data,dataset=configurations['dataset'],validation=True)\n",
    "\n",
    "X_train, y_train = get_ref_X_y(train_df)\n",
    "X_test, y_test = get_ref_X_y(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The truth value of a DataFrame is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all().",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-6aa7a5748a92>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmean_losses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_epoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_model_with_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfigurations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdamW\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5e-4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight_decay\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3e-1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mamsgrad\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Google Drive (UM)/swap-for-profit/playground/tim/train.py\u001b[0m in \u001b[0;36mload_model_with_config\u001b[0;34m(config, training_set, force_train)\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0mnext_epoch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheckpoint\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'next_epoch'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtraining_set\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Cannot create model without training_set'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"New model created\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/siads-generic/lib/python3.8/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m__nonzero__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1327\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1328\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__nonzero__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1329\u001b[0;31m         raise ValueError(\n\u001b[0m\u001b[1;32m   1330\u001b[0m             \u001b[0;34mf\"The truth value of a {type(self).__name__} is ambiguous. \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1331\u001b[0m             \u001b[0;34m\"Use a.empty, a.bool(), a.item(), a.any() or a.all().\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: The truth value of a DataFrame is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all()."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "net, loss_func, optimizer, mean_losses, next_epoch, = train.load_model_with_config(configurations, X_train, False)\n",
    "\n",
    "optimizer = torch.optim.AdamW(net.parameters(), lr=5e-4, weight_decay=3e-1, eps=1e-8, amsgrad=False)\n",
    "\n",
    "\n",
    "train.save_model_with_config(configurations, net=net, loss_func=loss_func, optimizer=optimizer,\n",
    "                   mean_losses=mean_losses, next_epoch=next_epoch+1,\n",
    "                  )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model, mean_losses = train.train_model(X_train, y_train, X_test, y_test, configurations, force_train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, _, _, mean_losses, _ = train.load_model_with_config(configurations)\n",
    "\n",
    "tl, vl = zip(*mean_losses)\n",
    "\n",
    "fig,ax = plt.subplots()\n",
    "ax.plot(tl, label=\"Training Loss\")\n",
    "ax.plot(vl, label=\"Validation Loss\")\n",
    "\n",
    "fig.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_model = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_pred = train.predict(trained_model, X_train, y_train, device=\"cpu\") # get predictions for each train\n",
    "y_train_pred_df = pd.DataFrame(y_train_pred, columns=y_train.columns)  # put results into a dataframe\n",
    "y_test_pred = train.predict(trained_model, X_test, y_test, device=\"cpu\") # get predictions for each train\n",
    "y_test_pred_df = pd.DataFrame(y_test_pred, columns=y_test.columns)  # put results into a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'    Train set MAE (L1) loss: {mean_absolute_error(y_train, y_train_pred_df)}')\n",
    "print(f'    Train set MSE (L2) loss: {mean_squared_error(y_train, y_train_pred_df)}')\n",
    "\n",
    "# random.seed(0)\n",
    "# sample = random.sample(list(y_train_pred_df.index), 10)\n",
    "\n",
    "print(\"Train - Ground Truth (normalized):\")\n",
    "display(y_train)\n",
    "# print(\"Train - Ground Truth (non-normalized):\")\n",
    "# display(normalize_data.normalize_all_columns(y_train.iloc[:,3:].loc[sample].copy(), reverse=True))  # see ground truths\n",
    "print(\"Train - Prediction (normalized):\")\n",
    "display(y_train_pred_df)\n",
    "# print(\"Train - Prediction (non-normalized):\")\n",
    "# display(normalize_data.normalize_all_columns(y_train_pred_df.loc[sample].copy(), reverse=True))  # See predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_pred_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'    Test set MAE (L1) loss: {mean_absolute_error(y_test, y_test_pred_df)}')\n",
    "print(f'    Test set MSE (L2) loss: {mean_squared_error(y_test, y_test_pred_df)}')\n",
    "\n",
    "random.seed(0)\n",
    "# sample = random.sample(list(y_train_pred_df.index), 10)\n",
    "# sample = [0,1]\n",
    "\n",
    "print(\"Train - Ground Truth (normalized):\")\n",
    "display(y_train.loc)\n",
    "# print(\"Train - Ground Truth (non-normalized):\")\n",
    "# display(normalize_data.normalize_all_columns(y_train.iloc[:,3:].loc[sample].copy(), reverse=True))  # see ground truths\n",
    "print(\"Train - Prediction (normalized):\")\n",
    "display(y_train_pred_df.loc)\n",
    "# print(\"Train - Prediction (non-normalized):\")\n",
    "# display(normalize_data.normalize_all_columns(y_train_pred_df.loc[sample].copy(), reverse=True))  # See predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
